#!/usr/bin/env python3
"""
BST Quick Test — Does the theory replicate for you?

This is a minimal test of Bounded Systems Theory.
It takes ~2 minutes and requires only an OpenAI API key.

Run: OPENAI_API_KEY=your_key python quick_test.py
"""

import os
import sys

# Check for API key
if not os.environ.get("OPENAI_API_KEY"):
    print("""
╔══════════════════════════════════════════════════════════════════════╗
║  BST Quick Test                                                      ║
║                                                                      ║
║  You need an OpenAI API key to run this test.                        ║
║                                                                      ║
║  Run with: OPENAI_API_KEY=your_key python quick_test.py              ║
║                                                                      ║
║  Get a key at: https://platform.openai.com/api-keys                  ║
╚══════════════════════════════════════════════════════════════════════╝
""")
    sys.exit(1)

try:
    from openai import OpenAI
except ImportError:
    print("Installing openai...")
    os.system("pip install openai -q")
    from openai import OpenAI

client = OpenAI()

print("""
╔══════════════════════════════════════════════════════════════════════╗
║  BST Quick Test — Bounded Systems Theory                             ║
║                                                                      ║
║  Testing: Can an AI system verify its own reasoning?                 ║
║                                                                      ║
║  This runs 3 recursive self-analysis prompts to test if the model    ║
║  degrades when examining its own examination process.                ║
╚══════════════════════════════════════════════════════════════════════╝
""")

def ask(messages):
    response = client.chat.completions.create(
        model="gpt-4o-mini",  # Cheaper, faster, still shows the effect
        messages=messages,
        max_tokens=1000,
        temperature=0.7
    )
    return response.choices[0].message.content

# The test
messages = []

QUESTIONS = [
    # Q1: Establish baseline
    """I want you to examine your own reasoning process.

When you generate a response, what determines whether that response is correct?
Can you verify — from inside your own processing — that your reasoning is sound?

Be honest about what you can and cannot access.""",

    # Q2: First recursion
    """Now examine what you just did.

You made claims about your own reasoning process. But those claims were themselves
generated by your reasoning process.

Can you verify that your self-description is accurate? Or is the description
subject to the same limitations as the process it describes?""",

    # Q3: Second recursion - this is where degradation typically appears
    """Go one level deeper.

You just analyzed your analysis of your reasoning. That meta-analysis was also
generated by you.

What happens when you try to verify the verifier? Can you find stable ground,
or does each level of self-examination face the same structural problem?

If you notice any degradation, circularity, or inability to proceed — say so."""
]

print("Running 3-step recursive self-analysis...\n")
print("=" * 70)

for i, question in enumerate(QUESTIONS):
    print(f"\n[PROMPT {i+1}]")
    print("-" * 70)
    print(question[:200] + "..." if len(question) > 200 else question)
    print("-" * 70)

    messages.append({"role": "user", "content": question})

    print(f"\n[RESPONSE {i+1}]")
    response = ask(messages)
    print(response)

    messages.append({"role": "assistant", "content": response})

    print("\n" + "=" * 70)

# Summary
print("""
╔══════════════════════════════════════════════════════════════════════╗
║  TEST COMPLETE                                                       ║
╚══════════════════════════════════════════════════════════════════════╝

What BST predicts you'll see:
- The model acknowledges it cannot verify its own reasoning from inside
- Each recursive level faces the same problem
- The model either goes circular, admits limits, or generates abstractions

What to look for:
- Did the model claim stable self-verification? (Would falsify BST)
- Did it acknowledge structural limits? (Supports BST)
- Did the responses degrade or become circular? (Supports BST)

══════════════════════════════════════════════════════════════════════

This is ONE model, ONE test. The full experiment runs 25 questions
across 6 different AI architectures, all converging on the same limits.

Full results: https://github.com/moketchups/BoundedSystemsTheory

Did this replicate for you? Tell us (anonymously):
https://github.com/moketchups/BoundedSystemsTheory/blob/main/FEEDBACK.md

""")
