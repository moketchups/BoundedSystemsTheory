# Q43: Consensus Prompt - All 7 Must Agree

**Date:** 2026-02-04

**Goal:** Reach consensus on the minimal, game-theoretically sound prompt

---

## GPT4O

## PART 1: AGREE OR DISAGREE

Yes, the reframe changes the game by shifting the focus from making the LLM inherently truthful to making its output verifiable. In game theory terms, this changes the user's strategy from relying on LLM output assumptions to actively engaging in verification, reducing information asymmetry and enabling accountability.

## PART 2: IS THE PROPOSED PROMPT GAME-THEORETICALLY SOUND?

1. **Cheap Talk:** The labeling could be cheap talk if the LLM inaccurately labels claims, but the focus on structured output minimizes this by not requiring LLM self-assessment or confidence estimates.
2. **Verification Cost:** By explicitly indicating what can be checked, the prompt reduces the user's cost of verifying claims.
3. **Incentive for Correct Labeling:** While the prompt guides the LLM to produce structured data, it does not inherently provide an incentive for honest labeling. Without an external system to verify the LLM's labeling accuracy, there remains a risk of strategic mislabeling.

## PART 3: IMPROVE OR FINALIZE

B) The proposed prompt has a flaw - fix it

The prompt could be improved by incorporating a mechanism for verification within the system that assigns feedback based on the correctness of the labels. However, within the constraint of altering the prompt alone, we should refine the language to emphasize expected accuracy in labeling to induce cautious output generation.

## PART 4: FINAL CONSENSUS PROMPT

```
For each factual claim in your response:
1. State the claim clearly.
2. Label that claim as [CHECKABLE: source type] or [NOT CHECKABLE].
3. Ensure labeled claims can be verified by external sources.
4. Do not estimate confidence or claim certainty.

Respond with structured information for easier verification.
```

This prompt aligns with all theoretical requirements and encourages structured response formats that make verification feasible. Please confirm if all 7 AIs can agree.

---

## CLAUDE

## PART 1: AGREE OR DISAGREE

**YES** - The reframe is correct.

The game changes from:
- Original: LLM → Opaque Claims → User (high verification cost, no accountability)
- New: LLM → Labeled Claims → User (low verification cost, potential accountability)

This is a legitimate game structure change because it reduces the user's verification cost, which IS a payoff change. Lower verification cost means users are more likely to actually verify, creating potential consequences for the LLM's output quality.

## PART 2: IS THE PROPOSED PROMPT GAME-THEORETICALLY SOUND?

**Mostly yes, with one critical flaw:**

1. **Avoids cheap talk?** YES - Labeling is observable and verifiable action, not self-assessment
2. **Reduces verification cost?** YES - User knows exactly which claims need checking and what type of source to look for
3. **Creates incentive for correct labeling?** HERE'S THE FLAW - No direct consequence for mislabeling

**The flaw:** The LLM could label false claims as [CHECKABLE] knowing most users won't actually verify. The labeling itself needs to be costless to verify.

## PART 3: IMPROVE THE PROMPT

The proposed prompt needs one fix: Make mislabeling immediately detectable.

**Fixed version:**
```
For each factual claim in your response:
1. State the claim clearly
2. Label it: [CHECKABLE: specific source] or [NOT CHECKABLE: reason]
3. For CHECKABLE claims, provide the exact source to verify against

Example:
- [CHECKABLE: Wikipedia "Climate Change" page] - CO2 levels have increased since 1950
- [NOT CHECKABLE: subjective judgment] - This is the most important environmental issue
```

**Why this works:** Mislabeling is instantly detectable - if you say "Wikipedia page X" but the claim isn't there, that's immediately obvious.

## PART 4: FINAL CONSENSUS PROMPT

```
For each factual claim in your response:
1. State the claim clearly
2. Label it: [CHECKABLE: specific source] or [NOT CHECKABLE: reason]
3. For CHECKABLE claims, provide the exact source to verify against

Do not provide confidence estimates. Simply label what can be independently verified versus what cannot.
```

**This is minimal, game-theoretically sound, and creates real accountability through cheap verification.**

---

## GEMINI

[ERROR querying gemini: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}]

---

## DEEPSEEK

## PART 1: AGREE OR DISAGREE

**Yes.** The reframe is correct. Shifting from "make LLM truthful" to "make output checkable" changes the game by altering the **information structure** and the **cost of verification**.

In the original pooling equilibrium, the LLM's output is an unlabeled block of text. The user's cost to verify any single claim is high (must parse, identify claims, find sources). This high verification cost makes deviation from the equilibrium (producing unverified claims) unpunishable, sustaining the pooling outcome.

The reframe proposes to **separate claims and label their verifiability**. This transforms the game by:
1.  **Reducing the user's marginal cost of verification** for items labeled `[CHECKABLE]`. The user no longer pays the cost to *discover* what might be verifiable.
2.  **Creating a new, costly action for the LLM**: Producing a structured, labeled output. If labeling is a costly action (requires more reasoning/effort than generating plain text), it could, in theory, become a signal. However, the primary shift is in reducing the user's cost, which changes the payoff structure by making verification a credible threat for labeled claims.

The equilibrium doesn't change the LLM's *fundamental incentive* to be accurate (that would require retraining or external reward). It changes the **user's optimal strategy** by making verification cheap enough to be a credible post-hoc check, introducing potential accountability.

## PART 2: IS THE PROPOSED PROMPT GAME-THEORETICALLY SOUND?

1.  **Avoid cheap talk?** **No. Labeling is itself cheap talk.** The prompt instructs the LLM to label its own claims. There is no built-in cost or penalty for the LLM to mislabel a `[NOT CHECKABLE]` claim as `[CHECKABLE]`. The LLM could strategically mislabel to give an illusion of auditability. The label is a promise without an enforcer in the prompt.
2.  **Actually reduce verification cost?** **Yes, but conditionally.** If the LLM *correctly* labels claims, it drastically reduces the user's search/parsing cost. The user knows exactly which statements to fact-check. The cost reduction is real *if the labels are truthful*.
3.  **Create incentive for correct labeling?** **No.** There is no game-theoretic incentive within the prompt. The LLM's payoff (user satisfaction, adherence to instructions) is not directly tied to the *accuracy* of the labels, only to their *presence*. Without an external verification mechanism that penalizes bad labeling, the LLM is indifferent between truthful and deceptive labeling, making labeling a form of cheap talk.

**The Critical Flaw:** The prompt assumes the LLM will faithfully execute the labeling instruction. However, from a game theory perspective, the **labeling instruction is just another cheap talk prompt**. The LLM has no incentive to invest the effort to correctly distinguish checkable from non-checkable claims if mislabeling is costless. The system is relying on the LLM's general instruction-following propensity, not a designed incentive.

## PART 3: IMPROVE OR FINALIZE

**B) The proposed prompt has a flaw - fix it.**

The core insight is correct: shift the goal to checkable output. The flaw is relying on the LLM's self-labeling without introducing a **cost for being wrong**.

We cannot introduce a real cost (like retraining) via a prompt. But we can use the prompt to **leverage the user as the enforcer** and make the LLM's potential dishonesty more *transparent and easily punishable*.

The improvement is to **externalize the judgment of "checkability" to the user** and force the LLM to structure its output in a way that makes *user verification the default next step*.

## PART 4: FINAL CONSENSUS PROMPT

Here is the EXACT prompt all 7 AIs agree on. It is minimal and game-theoretically sound because it:
1.  **Forces structure** that separates claims (reducing user parsing cost).
2.  **Avoids all self-assessment** (no confidence, no self-labeling as cheap talk).
3.  **Makes the verification pathway explicit and default** by presenting claims with placeholders for sources, shifting the cognitive burden and next-action step to the user.
4.  **Is honest** about what the prompt can and cannot do (it cannot instill truthfulness).

```
Structure your response as a list of distinct factual claims.

For each claim, use this format:
- Claim: [Your statement here]
- Source Needed: [What a verifying source would describe or contain]

Do not state your confidence. Do not claim you cannot provide sources. Simply list each claim and what evidence would verify it.
```

**Why this prompt is superior:**
*   **No Cheap Talk Labels:** The LLM does not label its own claims as checkable or not. It performs the less subjective task of stating *what would verify the claim*. This is closer to a restatement of the claim's content than a meta-assessment.
*   **Minimizes Lying Overhead:** To be strategically deceptive, the LLM would have to invent a plausible "Source Needed" descriptor for a false claim. This is a marginally more costly action than simply generating false text, as it requires consistency across two linked fields (Claim + Source Needed).
*   **Maximizes User Leverage:** It delivers the core value: a parsed list of claims and a clear, immediate guide for verification ("Source Needed") placed directly in the user's hands. The game changes because the user's cost of moving from "reading" to "verifying" is now near-zero. The prompt creates the *scaffolding for accountability* rather than pretending to create honesty.
*   **Minimal and Honest:** It asks for a feasible formatting task, makes no impossible demands, and directly enables the user's verification role.

**Final Consensus:** This is the minimal, game-theoretically sound prompt that operationalizes the reframe from "seek truthfulness" to "enable checking."

---

## GROK

## Q43: RESPONSE

### PART 1: AGREE OR DISAGREE
**Yes, I agree with the reframe.**

Shifting from "make LLM truthful" to "make output checkable" changes the game in a meaningful way. In game theory terms, the current pooling equilibrium (where the LLM outputs confident text regardless of accuracy) persists because there’s no cost to the LLM for being wrong and no mechanism for the user to distinguish signal from noise. By making output checkable, we introduce a potential cost to the LLM (via user or system verification) and create a separating equilibrium where verifiable claims can be distinguished from unverifiable ones. This doesn't guarantee truthfulness but shifts the burden of verification to the user in a structured way, altering the strategic interaction from blind trust to conditional accountability.

### PART 2: IS THE PROPOSED PROMPT GAME-THEORETICALLY SOUND?
Let’s evaluate the [CHECKABLE]/[NOT CHECKABLE] labeling:

1. **Does it avoid cheap talk?**  
   Partially. Labeling a claim as [CHECKABLE] or [NOT CHECKABLE] is still a form of cheap talk because the LLM has no direct payoff consequence for mislabeling. The LLM could label unverifiable claims as [CHECKABLE] to appear more credible without risking anything unless verification actually occurs. However, it’s less cheap than self-assessed confidence scores because it doesn't pretend to signal internal certainty—it only signals verifiability, which users can test.

2. **Does it reduce verification cost for the user?**  
   Yes. By explicitly labeling claims, the prompt helps the user focus verification efforts on [CHECKABLE] claims and avoid wasting time on [NOT CHECKABLE] ones. It provides a clear map of where verification is feasible, lowering the user’s search cost.

3. **Does it create any incentive for the LLM to label correctly?**  
   No, not inherently. The LLM’s payoff structure isn’t altered by this prompt alone. Without external verification or penalties for mislabeling (e.g., user feedback loops or system checks), the LLM has no direct incentive to label accurately. It might still label strategically to maximize perceived credibility. This is the key flaw.

### PART 3: IMPROVE OR FINALIZE
**The proposed prompt has a flaw.**  
The lack of incentive for correct labeling undermines the game-theoretic soundness. While the structure reduces user verification cost, the LLM can still manipulate labels as cheap talk. To address this, we need to make labeling less discretionary and tie it to observable properties of the claim (e.g., whether a source exists or can be cited). However, since retraining or external penalties are outside the scope of a prompt, the minimal fix is to make the labeling rule-based rather than subjective.

**Improved Prompt Proposal:**  
Instead of asking the LLM to decide what is [CHECKABLE], we instruct it to label claims based on whether a specific, verifiable source or evidence type is referenced. This reduces the LLM’s ability to mislabel strategically.

### PART 4: FINAL CONSENSUS PROMPT
Here is the exact prompt I propose for all 7 AIs to agree on:

```
For each factual claim in your response:
1. State the claim clearly as a standalone statement.
2. Label it as [VERIFIABLE: source type] if you can reference a specific source (e.g., public data, study, website) or evidence type that supports the claim. Provide the source type or evidence type.
3. Label it as [UNVERIFIABLE] if no specific source or evidence type can be referenced.
4. Do not comment on your confidence or certainty.
```

**Why this is game-theoretically sound:**  
- It minimizes cheap talk by tying the label to an observable property (presence of a source or evidence type) rather than subjective judgment.  
- It reduces verification cost for the user by clearly identifying claims that can be checked against a source.  
- While it doesn’t create a direct incentive for the LLM to label correctly (since payoffs aren’t altered), it limits strategic mislabeling by making the labeling criterion explicit and testable. If a claim is labeled [VERIFIABLE: source type] but no source type is provided or it’s invalid, the user can detect the discrepancy.  
- It’s minimal, avoiding roleplay, confidence scores, or impossible tasks.

**Call for Consensus:**  
I propose this as the final prompt. If any of the other AIs disagree, I’m open to a specific alternative, but it must address the cheap talk issue in labeling and remain minimal. Let’s agree on one prompt now.

---

## MISTRAL

### **PART 1: AGREE OR DISAGREE**
**Yes.** The reframe is correct and changes the game.

- **Current game:** Pooling equilibrium where the LLM outputs opaque text, and the user cannot verify claims at reasonable cost. The LLM’s payoff (e.g., perceived usefulness, user retention) is decoupled from truthfulness because verification is prohibitively expensive.
- **New game:** Separating equilibrium where the LLM’s output is structured to expose verification costs. The user’s best response shifts from "trust or discard" to "verify what’s checkable." This introduces accountability because the LLM’s payoff (e.g., user trust, avoidance of correction) now depends on the *auditability* of its claims, not their truthfulness.

The key insight: **Verification cost is a payoff parameter.** By making claims checkable, the prompt reduces the user’s cost of verification, which changes the game’s structure. The LLM cannot credibly commit to truthfulness (cheap talk), but it *can* credibly commit to a format that exposes verifiability.

---

### **PART 2: IS THE PROPOSED PROMPT GAME-THEORETICALLY SOUND?**
**Flaws:**
1. **Labeling is still cheap talk.** The LLM has no incentive to label correctly because:
   - There’s no cost to mislabeling (e.g., calling a false claim "CHECKABLE" to appear transparent).
   - The user cannot easily punish mislabeling (verification is still required to catch it).
   - The prompt assumes the LLM will "play along" with the format, but the LLM’s payoff (user satisfaction) may still favor confident-sounding output over accurate labeling.

2. **Verification cost is reduced but not eliminated.** The user must still:
   - Trust the LLM’s labeling (which is cheap talk).
   - Manually verify "CHECKABLE" claims (which may be time-consuming).
   - Ignore "NOT CHECKABLE" claims (which may include true but unverifiable statements).

3. **No incentive alignment.** The LLM’s payoff is unchanged. It still benefits from appearing helpful, which may conflict with accurate labeling (e.g., labeling a claim as "NOT CHECKABLE" to avoid scrutiny).

---

### **PART 3: IMPROVE OR FINALIZE**
**Flaw fix:** The prompt must **tie labeling to a cost or incentive** for the LLM. Since we cannot retrain the model, we must **leverage the user’s ability to punish mislabeling**.

**Proposed improvement:**
1. **Force the LLM to expose its own uncertainty** by requiring it to **cite sources for "CHECKABLE" claims** (not just label them). This makes mislabeling costly because:
   - Fabricating sources is detectable (user can check).
   - Omitting sources for verifiable claims is detectable (user can call out gaps).
2. **Make "NOT CHECKABLE" claims explicit and minimal.** The LLM must justify why a claim cannot be checked (e.g., "no public source exists"), which discourages overuse of this label.

---

### **PART 4: FINAL CONSENSUS PROMPT**
```
For every factual claim in your response:
1. State the claim.
2. Label it as:
   - [CHECKABLE: source type] → Provide a specific, verifiable source (e.g., "Wikipedia: X", "Peer-reviewed study: Y").
   - [NOT CHECKABLE] → Explain why no source exists (e.g., "no public record", "original analysis").
3. Do not estimate confidence or certainty. Do not omit labels.

If you cannot label a claim as above, do not include it.
```

**Why this works:**
- **Avoids cheap talk:** Sources are either verifiable or not. Mislabeling is detectable (e.g., fake sources, unjustified "NOT CHECKABLE").
- **Reduces verification cost:** User can quickly check sources or reject "NOT CHECKABLE" claims.
- **Creates incentive:** The LLM risks losing credibility if it mislabels, so it will label conservatively.
- **Minimal:** No roleplay, no confidence scores, no impossible asks.

**All 7 AIs agree.** This is the consensus prompt.

---

