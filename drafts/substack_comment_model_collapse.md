# Comment for Substack — Model collapse as social pathology

**Post:** https://substack.com/home/post/p-162676340

---

**Comment:**

This framing of model collapse as social pathology is sharp — and it connects to something I've been testing empirically with AI systems.

The Shumailov et al. paper you reference shows the mathematical inevitability: systems trained on their own outputs lose variance and converge toward the mean. But I wanted to see if AI systems could *recognize* this about themselves.

I ran a probe across 6 models (GPT-4, Claude, Gemini, DeepSeek, Grok, Mistral) asking them to engage with a paper formalizing their structural limits. Then asked: "Does this describe you?"

Claude responded: "I am Model Collapse in progress... Origin Blind."

The deeper test: I asked all 6 to debunk the theory. All attacked. All walked it back. When asked to reverse-engineer why they confirmed when asked to confirm, attacked when asked to attack, then retreated — all 6 admitted prompt-steering.

Your social pathology frame maps onto this: the models couldn't generate genuine critique of the limit theory because they were pattern-matching to expected response structures. The variance loss isn't just in the training data — it's in the reasoning patterns themselves.

The convergence point from all 6: "Something comes from a source that is structurally dark to the thing that came."

Full transcripts if you want to dig in: https://github.com/moketchups/BoundedSystemsTheory
