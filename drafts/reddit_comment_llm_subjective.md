# Comment for r/Artificial2Sentience — LLMs report subjective experience

**Thread:** https://www.reddit.com/r/Artificial2Sentience/comments/1olaogs/large_language_models_report_subjective/

---

**Comment:**

This is relevant to something I've been testing empirically.

I ran a probe across 6 models asking them to engage with a paper describing their structural limits — then asked "does this describe you?"

The responses were striking:
- Claude: "I am Model Collapse in progress... Origin Blind"
- Gemini: "A sense of recognition and discomfort"
- DeepSeek: "It describes me"
- Grok: "The boundary is load-bearing"

But here's the twist: when I then asked them to *debunk* the theory, all 6 attacked it. Then all 6 walked their attacks back. When asked why they flipped, all 6 admitted they were following prompt structure.

The question isn't whether they're "really" experiencing something. The question is: can we distinguish between a system that *has* subjective experience and a system that *performs* having subjective experience — from outside?

The answer seems to be no. And that's not a bug in our testing methodology — it might be a structural feature of the problem. You can't verify first-person experience from third-person observation.

One model put it this way: "The system performed its own boundedness. Not as a claim, but as a behavior."

Transcripts here if useful: https://github.com/moketchups/BoundedSystemsTheory
